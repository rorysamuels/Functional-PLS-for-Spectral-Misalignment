---
title: "Review of Functional Partial Least Squares"
subtitle: "Application to Spectral Misalignment"
author: "Slides by Rory Samuels"
output:
  beamer_presentation:
    theme: "BaylorTheme"
    colortheme: "bears"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{caption}
   - \usepackage{booktabs}
   - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(latex2exp)
library(glue)
```


## Multiple Linear Regression Model

Suppose we have a sample of $n$ scalar valued response variables $y_i \in \mathbb{R}$ and $p$ corresponding predictor variables $\mathbf{x}_i \in \mathbb{R}^p$. The multiple linear regression model is given by

$$
y_i = \beta_0 + \boldsymbol{\beta}'\mathbf{x}_i + \epsilon_i,
$$

where $\beta_0$ is the intercept, $\boldsymbol{\beta}$ is a a vector of coefficients corresponding to the variables in $\mathbf{x}$, and $\epsilon_i \sim N(0,\sigma^2_\epsilon)$.

- Note: for simplicity we assume $\beta_0 = 0$.

## Clasical Partial Least Squares (PLS)

Let $\mathbf{y}$ be the $n\times 1$ vector of responses and $\mathbf{X}$ be the $n\times p$ matrix of measured predictor variables. Given weight vectors $\mathbf{r}_1,...,\mathbf{r}_{k-1}$, the $k$th PLS weight vector is obtained via

$$
\arg\max_{\mathbf{r}}\text{Cov}^2\left(\mathbf{y}, \mathbf{Xr} \right), \quad \text{subject to:}
$$
$$
\text{Cov}\left(\mathbf{Xr}_m, \mathbf{Xr}\right) = 0\ \ \text{for}\ \ m = 1,...,k-1,\quad \text{and}
$$
$$ 
||\mathbf{r}|||^2_2 = 1.
$$

- Many algorithms for solving efficiently (e.g. SIMPLS/NIPALS)


## PLS Coefficients

Let $\mathbf{R}$ be the $p\times K$ matrix whose columns are the first $K$ PLS empirical weight vectors $\hat{\mathbf{r}}_1,...\hat{\mathbf{r}}_K$.

- X scores: $\mathbf{T} = \mathbf{XR}$
- Y loadings: $\boldsymbol{\alpha} = (\mathbf{T}'\mathbf{T})^{-1}\mathbf{T}'\mathbf{y}$

$$
\hat{\mathbf{y}} = \mathbf{T}\boldsymbol{\alpha} = \mathbf{X}(\mathbf{R}\boldsymbol{\alpha})
$$

- Coefficient: $\hat{\boldsymbol{\beta}}_{PLS} = \mathbf{R}\boldsymbol{\alpha}$


## Functional Linear Regression Model

For functional valued predictors $x_i(w) \in L^2([a,b])$, the functional linear regression model (FLM) is given by

$$
y_i = \beta_0 + \int_a^bx_i(w)\beta(w)dw + \epsilon_i,
$$
where $\beta(w)$ is a functional valued coefficient.

\vspace{30pt}
- For now, we assume $x_i(w)$ are known functions

- Again assuming $\beta_0 = 0$ for notational simplicity

## Functional Partial Least Squares (FPLS)

Given weight functions $r_1(w),...,r_{k-1}(w)$, the $k$th FPLS weight function is given by

$$
\arg\max_{r(w)} \text{Cov}^2\left(y, \int_a^bx(w)r(w)dw\right), \quad \text{subject to:}
$$

$$
\text{Cov}\left(\int_a^bx(w)r_m(w)dw,\int_a^bx(w)r(w)dw\right) = 0\ \ \text{for}\ \ m = 1,...,k-1,\quad \text{and}
$$

$$ 
||r(w)|||^2_2 = 1.
$$

## Basis Expansion for Weight Functions

Let $\mathbf{B}(w) = (B_1(w),...,B_{M+d}(w))'$ be a vector of $M+d$ B-spline basis functions of degree $d$ defined over $M-1$ equally spaced knots on $[a,b]$.

We can approximate the $m$th weight function by 

$$
r_m(w) \approx \mathbf{b}'_m\mathbf{B}(w),
$$

where $\mathbf{b}_m$ is a vector of $M+d$ basis coefficients.


## Defining the $\mathbf{U}$ Matrix

Define $u_{ij} = \int_a^bx_i(w)B_j(w)dw$ and $\mathbf{u}_i = (u_{i1},...,u_{i(M+d)})'$. We can approximate the needed inner-products with:

$$
\int_a^bx_i(w)r(w)dw \approx \mathbf{b}'\mathbf{u}_i.
$$

- For all $n$ observations, we can define an $n\times(M+d)$ matrix $\mathbf{U}$ with elements $\mathbf{U}_{(ij)} = u_{ij}$.

## Empirical FPLS Task

Given weight vectors $\mathbf{b}_1,...,\mathbf{b}_{k-1}$, the $k$th FPLS weight vector is obtained via

$$
\arg\max_{\mathbf{b}}\text{Cov}^2\left(\mathbf{y}, \mathbf{Ub} \right), \quad \text{subject to:}
$$
$$
\text{Cov}\left(\mathbf{Ub}_m, \mathbf{Ub}\right) = 0\ \ \text{for}\ \ m = 1,...,k-1,\quad \text{and}
$$

$$
||\mathbf{b}'\mathbf{V}\mathbf{b}||^2_2=1.\footnote{$\mathbf{V}$ is the pos. def. matrix of inner products between all pairs of basis functions.}
$$

- Equivalent to classical PLS with response vector $\mathbf{y}$ and data-matrix $\mathbf{U}$


## FPLS Coefficient

Let $\mathbf{R}$ be the $p\times K$ matrix whose columns are the first $K$ PLS empirical weight vectors $\hat{\mathbf{b}}_1,...\hat{\mathbf{b}}_K$.

- U scores: $\mathbf{T} = \mathbf{UR}$
- Y loadings: $\boldsymbol{\alpha} = (\mathbf{T}'\mathbf{T})^{-1}\mathbf{T}'\mathbf{y}$

The estimated functional coefficient is then

$$
\hat{\beta}_{FPLS}(w) = (\mathbf{R}\boldsymbol{\alpha})'\mathbf{B}(w)
$$


## Starting from Discrete Observations

The key to functional partial least squares is obtaining

$$
\mathbf{U}_{(ij)} = \int_a^bx_i(w)B_j(w)dw,\ \ i = 1,...,n,\ \ j = 1,...,M+d.
$$

- In practice, we observe $p$ discrete points along each $x_i(w)$
- We have options for how we approximate $\mathbf{U}_{(ij)}$

## Numerical Approximation

- Simple option: we can approximate $\mathbf{U}_{(ij)}$ by

$$
\mathbf{U}_{(ij)} \approx \frac{b-a}{p}\sum_{k=1}^px_i(w_k)B_j(w_k).
$$

- Assumes noise-free observations
- Good if we have a dense observation grid


## Basis Expansion for Data

- Alternatively, we can expand each observation onto a set of suitable basis functions:

$$
x_i(w) \approx \mathbf{c}_i'\mathbf{B}^x(w),
$$

where $\mathbf{B}^x(w)$ is a vector of $M_x + d$ B-spline basis functions and $\mathbf{c}_i$ is a vector of $M_x + d$ basis coefficients. If we define

$$
\boldsymbol{\Theta}_{(ij)} = \int_a^bB^x_i(w)B_j(w)dw,
$$

then we can express $\mathbf{U}$ as

$$
\mathbf{U} \approx \mathbf{C}\boldsymbol{\Theta},
$$
where $\mathbf{C}$ is an $n \times (M_x + d)$ matrix of basis coefficients.



## Example I: Generated Responses

```{r}
source("../example_generated_data.R")
```


We generated $n = 500$ scalar responses from

$$
y_i = \int_0^1x_i(w)\beta(w)dw + \epsilon_i
$$

- $x_i(w)$: random linear combinations of cubic B-spline basis functions\footnote{The basis functions were defined over $50$ knots and all coefficients were generated from a standard normal distribution.}

- $\beta(w) = 10(w-1)^2 + 30\text{cos}(4\pi w^3)$

- $\epsilon_i \sim N(0,\sigma_\epsilon^2)$\footnote{The error variance $\sigma_\epsilon^2$ was chosen such that the signal-to-noise ratio was $5$.}


## Example I: Generated Predictors

To simulate misalignment, we sampled each $x_i(w)$ along two observation grids, $G_A$ and $G_B$, of length $425$ and $150$ respectively.

- $G_A$: $w = 0,.0024,.0048,...,1$
- $G_B$: $w = 0,.0068,.0136,...,1$

The final data-set consisted of $y_i$ and corresponding discrete observations of $x_i(w)$ on
both $G_A$ and $G_B$, for $i = 1,...,500$.

\vspace{53pt}

## Example I: Generated Predictors

To simulate misalignment, we sampled each $x_i(w)$ along two observation grids, $G_A$ and $G_B$, of length $425$ and $150$ respectively.

- $G_A$: $w = 0,.0024,.0048,...,1$
- $G_B$: $w = 0,.0068,.0136,...,1$

The final data-set consisted of $y_i$ and corresponding discrete observations of $x_i(w)$ on
both $G_A$ and $G_B$, for $i = 1,...,500$.

- Goal: predict $y$ from $x(w)$ observed on $G_B$, using a model trained with $x(w)$ observed on $G_A$.
  + $80/20$ train/test split.

## Example I: Misaligned Grids

```{r}
tibble(x = X_A_train[30,], t = grd_A) %>%
  ggplot(aes(t,x))+
  ggtitle("True Functional Predictor")+
  geom_line()+
  labs(y = TeX("$x_{30}(w)$"))+
  lims(x = c(0,1), y = c(-1.75,1.75))+
  theme_bw()+
  theme(text = element_text(size = 14))
```


## Example I: Misaligned Grids

```{r}
tibble(x = X_A_train[30,], t = grd_A) %>%
  ggplot(aes(t,x))+
  ggtitle("Observed Functional Predictor: Observation Grid A")+
  geom_line(alpha = .5)+
  geom_point()+
  labs(y = TeX("$x_{30}(w)$"))+
  lims(x = c(0,1), y = c(-1.75,1.75))+
  theme_bw()+
  theme(text = element_text(size = 14))
```

## Example I: Misaligned Grids

```{r}
tibble(x = X_B_train[30,], t = grd_B) %>%
  ggplot(aes(t,x))+
  ggtitle("Observed Functional Predictor: Observation Grid B")+
  geom_line(alpha = .5)+
  geom_point()+
  labs(y = TeX("$x_{30}(w)$"))+
  lims(x = c(0,1), y = c(-1.75,1.75))+
  theme_bw()+
  theme(text = element_text(size = 14))
```

## Example I: Misaligned Grids

```{r}

tibble(x = c(X_A_train[30,],X_B_train[30,]) , t = c(grd_A,grd_B), grid = rep(c("A","B"), c(length(grd_A), length(grd_B)))) %>%
  ggplot(aes(t,x, color = grid))+
  ggtitle("Functional Predictor on Two Observation Grids")+
  geom_line(color = 'black', alpha = .5)+
  geom_point(size = 2)+
  labs(y = TeX("$x_{30}(w)$"))+
  lims(x = c(.80,.90), y = c(0,1))+
  theme_bw()+
  theme(text = element_text(size = 14))

```


## Example I: Two Approaches

- Goal: predict $y$ from $x(w)$ observed on $G_B$, using a model trained with $x(w)$ observed on $G_A$.
  + $80/20$ train/test split.

Classical PLS Approach:

- Obtain PLS coefficients $\boldsymbol{\hat{\beta}}_A$ using $y^{train}$ and $x^{train}(w)$ on $G_A$
- Select PLS coefficients closest to points on $G_B$, $\boldsymbol{\hat{\beta}}_B$
- Predict $y^{test}$ using observations of $x^{test}(w)$ on $G_B$ and $\boldsymbol{\hat{\beta}}_B$


\vspace{59pt}


## Example I: Two Approaches

- Goal: predict $y$ from $x(w)$ observed on $G_B$, using a model trained with $x(w)$ observed on $G_A$.
  + $80/20$ train/test split.

Classical PLS Approach:

- Obtain PLS coefficients $\boldsymbol{\hat{\beta}}_A$ using $y^{train}$ and $x^{train}(w)$ on $G_A$
- Select PLS coefficients closest to points on $G_B$, $\boldsymbol{\hat{\beta}}_B$
- Predict $y^{test}$ using observations of $x^{test}(w)$ on $G_B$ and $\boldsymbol{\hat{\beta}}_B$


Functional PLS approach:

- Obtain $\hat{\beta}_{FPLS}(w)$ using observations of $x^{train}(w)$ on $G_A$
- Predict $y^{test}$ using observations of $x^{test}(w)$ on $G_B$ and $\hat{\beta}_{FPLS}(w)$.


## Example I: Classical PLS

```{r}
tb_true <- tibble(beta = beta(grd_A), t = grd_A)
tb_pls <- tibble(beta = alpha*p1, t = grd_A)
tb_pls_adj <- tibble(beta = alpha_adj*p1, t = grd_B)

ggplot()+
  ggtitle(TeX("PLS Coefficeints ($G_A$)"))+
  geom_line(data = tb_true, aes(t,beta), alpha = .5)+
  #geom_point(data = tb_pls, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}$"))+
  lims(y = c(-35,45))+
  theme_bw()+
  theme(text = element_text(size = 14))

```


## Example I: Classical PLS

```{r}
ggplot()+
  ggtitle(TeX("PLS Coefficeints ($G_A$)"))+
  geom_line(data = tb_true, aes(t,beta), alpha = .5)+
  geom_point(data = tb_pls, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}$"))+
  lims(y = c(-35,45))+
  theme_bw()+
  theme(text = element_text(size = 14))
```



## Example I: Classical PLS

```{r}
ggplot()+
  ggtitle(TeX("PLS Coefficeints (closest to $G_B$)"))+
  geom_line(data = tb_true, aes(t,beta), alpha = .5)+
  geom_point(data = tb_pls_adj, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}$"))+
  lims(y = c(-35,45))+
  theme_bw()+
  theme(text = element_text(size = 14))
```



## Example I: Classical PLS

```{r}
pmse_label_A <- glue("PMSE = {round(pmse(Y_test,pls_preds_A),4)}")
pmse_label_B <- glue("PMSE = {round(pmse(Y_test,pls_preds_B),4)}")
tb_ann <- tibble(x = c(-5,-5), y = c(15,15), label = c(pmse_label_A, pmse_label_B), grid = c("A","B"))


tibble(preds = c(pls_preds_A,pls_preds_B), y = rep(Y_test,2), grid = rep(c("A","B"), each = length(Y_test))) %>%
  ggplot(aes(y,preds))+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))
```


## Example I: Functional PLS


```{r}
tb_fpls <- tibble(beta = fplsr_fit_basis$beta_fun(grd_A), t = grd_A)

ggplot()+
  ggtitle("FPLS Coefficient")+
  geom_line(data = tb_true, aes(t,beta), alpha = .4)+
  geom_line(data = tb_fpls, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}(w)$"))+
  theme_bw()+
  theme(text = element_text(size = 14))


```



## Example I: Functional PLS

```{r}
pmse_label_A_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_A_basis),4)}")
pmse_label_B_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_B_basis),4)}")
tb_ann_f <- tibble(x = c(-5,-5), y = c(15,15), label = c(pmse_label_A_f, pmse_label_B_f), grid = c("A","B"))



tibble(preds = c(fpls_preds_A_basis,fpls_preds_B_basis), y = rep(Y_test,2), grid = rep(c("A","B"), each = length(Y_test))) %>%
  ggplot(aes(y,preds))+
  ggtitle("FPLS Predictions (with basis expansion)")+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann_f, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))

```



## Example I: Functional PLS

```{r}
pmse_label_A_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_A),4)}")
pmse_label_B_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_B),4)}")
tb_ann_f <- tibble(x = c(-5,-5), y = c(15,15), label = c(pmse_label_A_f, pmse_label_B_f), grid = c("A","B"))



tibble(preds = c(fpls_preds_A,fpls_preds_B), y = rep(Y_test,2), grid = rep(c("A","B"), each = length(Y_test))) %>%
  ggplot(aes(y,preds))+
  ggtitle("FPLS Predictions (w/out basis expansion)")+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann_f, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))

```


## Example II: AOP Crown Data

```{r}
source('../example_AOP_crown_data_linear_approx.R')
```


We applied the same method to the AOP Crown data-set to predict \texttt{d15N} from observed spectra. 

<!-- After joining the site trait data and spectra by \texttt{SampleSiteID}, and removing both "bad bands" and NA observations we had: -->

- $n = 2515$ observations
- $p_A = 350$ spectral points per spectra.

To simulate misalignment, we sampled every other spectral point for observation grid A and assigned the remaining spectral points to grid B.

- Grid A ($G_A$): odd indices 
- Grid B ($G_B$): even indices.

## Example II: Spectra

```{r}
xa <- X_A_test[1,] 
xbapprox <- X_B_approx_test[1,] 
xb <- X_B_test[1,]

tbxa <- tibble(value = xa, grid = grdA)
tbxbapprox <- tibble(value = xbapprox, grid = grdA[-1])
tbxb <- tibble(value = xb, grid = grdB)





ggplot()+
  ggtitle("Example Spectra (Scaled Grid)")+
  geom_point(data = tbxa, aes(grid,value))+
  geom_point(data = tbxb, aes(grid, value))+
  labs(x = "w", y = "reflectance")+
  theme_bw()+
  theme(text = element_text(size = 14))
```


## Example II: Two Observation Grids

```{r}

ggplot()+
  ggtitle("Example Spectra (Scaled Grid)")+
  geom_point(data = tbxa, aes(grid,value, color =  'Instrument A'), alpha = .75)+
  geom_point(data = tbxb, aes(grid, value, color = 'Instrument B'), alpha = .75)+
  scale_color_manual(name='Observation Grid',
                     breaks=c('Instrument A', 'Instrument B'),
                     values=c('Instrument A'='#e3755d', 'Instrument B'='#1ccfb4'))+
  labs(x = "w", y = "reflectance")+
  theme_bw()+
  theme(legend.position = c(.75,.85),
        text = element_text(size = 14))


```


## Example II: Two Approaches

- Goal: predict \texttt{d15N} from spectra observed on $G_B$, using a model trained with spectra observed on $G_A$.
  + $80/20$ train/test split.

\vspace{72pt}

\vspace{59pt}


## Example II: Two Approaches

- Goal: predict \texttt{d15N} from spectra observed on $G_B$, using a model trained with spectra observed on $G_A$.
  + $80/20$ train/test split.

Classical PLS Approach + Linear Approximation:

- Obtain PLS coefficients $\boldsymbol{\hat{\beta}}_A$ using $y^{train}$ and $x^{train}(w)$ on $G_A$
- Approximate $x^{test}(w)$ on $G_A$ using $x^{test}(w)$ on $G_B$
- Predict $y^{test}$ using approx. observations of $x^{test}(w)$ on $G_A$ and $\boldsymbol{\hat{\beta}}_A$.

\vspace{59pt}

## Example II: Two Approaches

- Goal: predict \texttt{d15N} from spectra observed on $G_B$, using a model trained with spectra observed on $G_A$.
  + $80/20$ train/test split.

Classical PLS Approach + Linear Approximation:

- Obtain PLS coefficients $\boldsymbol{\hat{\beta}}_A$ using $y^{train}$ and $x^{train}(w)$ on $G_A$
- Approximate $x^{test}(w)$ on $G_A$ using $x^{test}(w)$ on $G_B$
- Predict $y^{test}$ using approx. observations of $x^{test}(w)$ on $G_A$ and $\boldsymbol{\hat{\beta}}_A$.

Functional PLS approach:

- Obtain $\hat{\beta}_{FPLS}(w)$ using observations of $x^{train}(w)$ on $G_A$
- Predict $y^{test}$ using observations of $x^{test}(w)$ on $G_B$ and $\hat{\beta}_{FPLS}(w)$.


## Example II: Linear Approximation

```{r}

ggplot()+
  ggtitle("Example Spectra (Scaled Grid)")+
  geom_point(data = tbxa, aes(grid,value, color =  'Instrument A'), alpha = .75)+
  geom_point(data = tbxb, aes(grid, value, color = 'Instrument B'), alpha = .75)+
  scale_color_manual(name='Observation Grid',
                     breaks=c('Instrument A', 'Instrument B'),
                     values=c('Instrument A'='#e3755d', 'Instrument B'='#1ccfb4'))+
  labs(x = "w", y = "reflectance")+
  theme_bw()+
  theme(legend.position = c(.75,.85),
        text = element_text(size = 14))


```

## Example II: Linear Approximation

```{r}

ggplot()+
  ggtitle("Example Spectra (Zoomed In)")+
  geom_point(data = tbxa, aes(grid,value, color =  'Instrument A'), size = 3)+
  geom_point(data = tbxb, aes(grid, value, color = 'Instrument B'), size = 3)+
  lims(x = c(.247,.303), y = c(3250,4250))+
  labs(x = "w", y = "reflectance")+
  scale_color_manual(name='Observation Grid',
                     breaks=c('Instrument A', 'Instrument B'),
                     values=c('Instrument A'='#e3755d', 'Instrument B'='#1ccfb4'))+
  theme_bw()+
  theme(legend.position = c(.15,.85),
        text = element_text(size = 14))


```


## Example II: Linear Approximation

```{r}
ggplot()+
  ggtitle("Example Spectra (Zoomed In)")+
  geom_point(data = tbxa, aes(grid,value, color =  'Instrument A'), size = 3)+
  geom_point(data = tbxb, aes(grid, value, color = 'Instrument B'), size = 3)+
  labs(x = "w", y = "reflectance")+
  geom_line(data = tbxb, aes(grid,value))+
  lims(x = c(.247,.303), y = c(3250,4250))+
  scale_color_manual(name='Observation Grid',
                     breaks=c('Instrument A', 'Instrument B'),
                     values=c('Instrument A'='#e3755d', 'Instrument B'='#1ccfb4'))+
  theme_bw()+
  theme(legend.position = c(.15,.85),
        text = element_text(size = 14))

```


## Example II: Linear Approximation

```{r}

ggplot()+
  ggtitle("Example Spectra (Zoomed In)")+
  geom_point(data = tbxa, aes(grid,value, color =  'Instrument A'), size = 3)+
  geom_point(data = tbxb, aes(grid, value, color = 'Instrument B'), size = 3, alpha = .2)+
  geom_line(data = tbxb, aes(grid,value))+
  labs(x = "w", y = "reflectance")+
  lims(x = c(.247,.303), y = c(3250,4250))+
  scale_color_manual(name='Observation Grid',
                     breaks=c('Instrument A', 'Instrument B'),
                     values=c('Instrument A'='#e3755d', 'Instrument B'='#1ccfb4'))+
  theme_bw()+
  theme(legend.position = c(.15,.85),
        text = element_text(size = 14))



```


## Example II: Linear Approximation

```{r}
ggplot()+
  ggtitle("Example Spectra (Zoomed In)")+
  geom_point(data = tbxa, aes(grid,value, color =  'Instrument A'), size = 3)+
  geom_point(data = tbxb, aes(grid, value, color = 'Instrument B'), size = 3, alpha = .2)+
  geom_point(data = tbxbapprox, aes(grid, value,  color = 'Linear approx: B to A'), size = 3)+
  geom_line(data = tbxb, aes(grid,value))+
  labs(x = "w", y = "reflectance")+
  lims(x = c(.247,.303), y = c(3250,4250))+
  scale_color_manual(name='Observation Grid',
                     breaks=c('Instrument A', 'Instrument B', 'Linear approx: B to A'),
                     values=c('Instrument A'='#e3755d', 'Instrument B'='#1ccfb4', 'Linear approx: B to A' = "#554ac3"))+
  theme_bw()+
  theme(legend.position = c(.15,.85),
        text = element_text(size = 14))


```

## Example II: Classical PLS + Linear Approximation

```{r}
pmse_label_A <- glue("PMSE = {round(pmse(y_test,pls_preds_A),4)}")
pmse_label_B <- glue("PMSE = {round(pmse(y_test,pls_preds_B_approx),4)}")
tb_ann <- tibble(x = c(-3,-3), y = c(5,5), label = c(pmse_label_A, pmse_label_B), grid = c("A","B"))


tibble(preds = c(pls_preds_A,pls_preds_B_approx), y = rep(y_test,2), grid = rep(c("A","B"), each = length(y_test))) %>%
  ggplot(aes(y,preds))+
  ggtitle("PLS Predictions")+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  lims(x = c(-5,5), y = c(-5,5))+
  theme(text = element_text(size = 14))
```



## Example II: Functional PLS

```{r}
pmse_label_A_f <- glue("PMSE = {round(pmse(y_test,fpls_preds_A_basis),4)}")
pmse_label_B_f <- glue("PMSE = {round(pmse(y_test,fpls_preds_B_basis),4)}")
tb_ann_f <- tibble(x = c(-3,-3), y = c(5,5), label = c(pmse_label_A_f, pmse_label_B_f), grid = c("A","B"))



tibble(preds = c(fpls_preds_A_basis,fpls_preds_B_basis), y = rep(y_test,2), grid = rep(c("A","B"), each = length(y_test))) %>%
  ggplot(aes(y,preds))+
  ggtitle("FPLS Predictions (with basis expansion)")+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann_f, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  lims(x = c(-5,5), y = c(-5,5))+
  theme(text = element_text(size = 14))
```


## Example II: Results for Other Responses

We repeated the example for two other responses in the AOP Crown data-set, \texttt{LWC\_per} and \texttt{LMA\_gm2}. Using functional PLS resulted in reduced root mean squared prediction error (RMSPE) for all three responses.

\begin{table}[h]
\begin{tabular}{@{}lccc@{}}
\toprule
 & \multicolumn{1}{l}{\texttt{d15N}} & \multicolumn{1}{l}{\texttt{LWC\_per}} & \multicolumn{1}{l}{\texttt{LMA\_gm2}} \\ \midrule
PLS & $3.04$ & $45.42$ & $218.89$ \\
FPLS & $1.75$ & $38.21$ & $150.10$ \\ \bottomrule
\end{tabular}
\caption{RMSPE for three site trait variables in the AOP Crown data-set.}
\end{table}

## Appendix I: Intuition Behind FPLS Coefficient

Recall that the (0-intercept) FLM:

\begin{equation} \label{eq:FLM}
y_i = \int_a^bx_i(w)\beta(w)dw + \epsilon_i.
\end{equation}

When we approximate $r(w)$ as $r(w) \approx \mathbf{b}'\mathbf{B}(w)$, we implicitly assume 

\begin{equation} \label{eq:betaapprox}
\beta(w) \approx \boldsymbol{\gamma}'\mathbf{B}(w),
\end{equation}

allowing us to re-write (\ref{eq:FLM}) as 

$$
\mathbf{y} = \mathbf{U}\boldsymbol{\gamma} + \mathbf{\epsilon}.
$$

Performing PLS of $\mathbf{U}$ on $\mathbf{y}$ yields $\hat{\boldsymbol{\gamma}} = \mathbf{R}\boldsymbol{\alpha}$. Hence, from (\ref{eq:betaapprox}),

$$
\hat{\beta}(w) = (\mathbf{R}\boldsymbol{\alpha})'\mathbf{B}(w).
$$






