---
title: "Review of Functional Partial Least Squares"
subtitle: "Application to Spectral Misalignment"
author: "Slides by Rory Samuels"
output:
  beamer_presentation:
    theme: "BaylorTheme"
    colortheme: "bears"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{caption}
   - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(latex2exp)
library(glue)
```


## Functional Linear Regression Model

Suppose we have sample of scalar valued response variables $y_i \in \mathbb{R}$ and functional valued predictors $x_i(t) \in L^2([a,b])$, for $i = 1,..,n$. The functional linear regression model (FLM) is given by:


$$
y_i = \beta_0 + \int_a^bx_i(t)\beta(t)dt + \epsilon_i,
$$

where $\beta_0$ is the intercept, $\beta(t)$ is the functional coefficient, and $\epsilon_i \sim N(0,\sigma^2)$.
\newline

- Note: for notational simplicity we assume $\beta_0 = 0$.

## Functional Partial Least Squares (FPLS)

Given weight functions $w_1(t),...,w_{k-1}(t)$, the $k$th weight function is obtained via

$$
\arg\max_{w(t)} \text{Cov}^2\left(y, \int_a^bx(t)w(t)dt\right).
$$

subject to: 

$$
\text{Cov}\left(\int_a^bx(t)w_j(t)dt,\int_a^bx(t)w(t)dt\right) = 0\ \ \text{for}\ \ j = 1,...,k-1,
$$

$$
\text{and }||w(t)||^2_2=1.
$$


## Basis Approximation for Weight Functions

Let $\mathbf{B}(t) = (B_1(t),...,B_{M+d}(t))'$ be a vector of $M+d$ B-spline basis functions of degree $d$. We can approximate the $j$th weight function by 

$$
w_j(t) \approx \mathbf{b}'_j\mathbf{B}(t),
$$

where $\mathbf{b}_j$ is a vector of $M+d$ basis coefficients.

- If we let $u_r = \int_a^bx(t)B_r(t)dt$ and $\mathbf{u} = (u_1,...,u_{M+d})'$ then

$$
\int_a^bx(t)w_j(t)dt \approx \mathbf{u}'\mathbf{b}_j
$$

## Basis Approximation for FPLS

\vspace{-8pt}
Given $\mathbf{b}_1,...,\mathbf{b}_{k-1}$, the $k$th weight vector is obtained via
\vspace{15pt}
$$
\arg\max_{\mathbf{b}} \text{Cov}^2\left(y, \mathbf{u}'\mathbf{b}\right).
$$

subject to:
$$
\text{Cov}\left(\mathbf{u}'\mathbf{b}_j,\mathbf{u}'\mathbf{b}\right) = 0\ \ \text{for}\ \ j = 1,...,k-1,
$$

$$
\text{and }||\mathbf{b}'\mathbf{V}\mathbf{b}||^2_2=1.\footnote{$\mathbf{V}$ is the pos. def. matrix of inner products between all pairs of basis functions.}
$$


## FPLS (Empirically)

Let $\mathbf{y}$ be a vector of $n$ observations of the response, and $\mathbf{U}$ be the $n \times (M+d)$ matrix with elements

$$
\mathbf{U}_{(ij)} = \int_a^bx_i(t)B_j(t)dt.
$$

- Finding optimal $\mathbf{b}_j$'s is equivalent to performing classical PLS with response $\mathbf{y}$ and covariates $\mathbf{U}$.
\newline
- Can be done efficiently with existing algorithms (e.g. SIMPLS or NIPALS).

## FPLS Coefficient

Let $\mathbf{R}$ be the $(M+d) \times K$ matrix whose columns are the first $K$ empirical weight vectors $\hat{\mathbf{b}}_1,...,\hat{\mathbf{b}}_K$.

- U Scores: $\mathbf{T} = \mathbf{UR}$
- Y loadings: $\mathbf{q} = (\mathbf{T}'\mathbf{T})^{-1}\mathbf{T}'\mathbf{y}$

The estimated functional coefficient is then

$$
\hat{\beta}_{FPLS}(t) = (\mathbf{Rq})'\mathbf{B}(t)
$$


## Starting from Discrete Observations

The key to functional partial least squares is obtaining

$$
\mathbf{U}_{(ij)} = \int_a^bx_i(t)B_j(t)dt,\ \ i = 1,...,n,\ \ j = 1,...,M+d.
$$

- In practice, we observe $p$ discrete points along each $x_i(t)$ (possibly with noise).

## Numerical Approximation

- If we have a dense observation grid, and negligible instrument noise, we can approximate $\mathbf{U}_{(ij)}$ by

$$
\mathbf{U}_{(ij)} \approx \frac{b-a}{p}\sum_{k=1}^px_i(t_k)B_j(t_k).
$$

## Basis Expansion for Data

- Alternatively, we can expand each observation onto a set of suitable basis functions:

$$
x_i(t) \approx \mathbf{c}_i'\mathbf{B}^x(t),
$$

where $\mathbf{B}^x(t)$ is a vector of $M_x + d$ B-spline basis functions and $\mathbf{c}_i$ is a vector of $M_x + d$ basis coefficients. In this case

$$
\mathbf{U}_{(ij)} \approx \mathbf{C}\boldsymbol{\Theta},
$$
where $\mathbf{C}$ is an $n \times (M_x + d)$ matrix of basis coefficients and $\boldsymbol{\Theta}$ is an $(M_x + d) \times (M + d)$ matrix with elements

$$
\boldsymbol{\Theta}_{(ij)} = \int_a^bB^x_i(t)B_j(t)dt.
$$


## Example I: Generated Responses

```{r}
source("../example_generated_data.R")
```


We generated $n = 500$ scalar responses from

$$
y_i = \int_0^1x_i(t)\beta(t)dt + \epsilon_i
$$

- $x_i(t)$: random linear combinations of cubic B-spline basis functions\footnote{The basis functions were defined over $50$ knots and all coefficients were generated from a standard normal distribution.}

- $\beta(t) = 10(t-1)^2 + 30\text{cos}(4\pi t^3)$

- $\epsilon_i \sim N(0,\sigma_\epsilon^2)$\footnote{The error variance $\sigma_\epsilon^2$ was chosen such that the signal-to-noise ratio was $5$.}


## Example I: Generated Predictors

To simulate misalignment, we sampled each $x_i(t)$ along two observation grids, $G_A$ and $G_B$, of length $425$ and $150$ respectively.

- $G_A$: $t = 0,.0024,.0048,...,1$
- $G_B$: $t = 0,.0068,.0136,...,1$

The final data-set consisted of $y_i$ and corresponding discrete observations of $x_i(t)$ on 
both $G_A$ and $G_B$, for $i = 1,...,500$.

\vspace{53pt}

## Example I: Generated Predictors

To simulate misalignment, we sampled each $x_i(t)$ along two observation grids, $G_A$ and $G_B$, of length $425$ and $150$ respectively.

- $G_A$: $t = 0,.0024,.0048,...,1$
- $G_B$: $t = 0,.0068,.0136,...,1$

The final data-set consisted of $y_i$ and corresponding discrete observations of $x_i(t)$ on 
both $G_A$ and $G_B$, for $i = 1,...,500$.

- Goal: predict $y$ from $x(t)$ observed on $G_B$, using a model trained with $x(t)$ observed on $G_A$.
  + $80/20$ train/test split.

## Example I: Misaligned Grids

```{r}
tibble(x = X_A_train[30,], t = grd_A) %>%
  ggplot(aes(t,x))+
  ggtitle("True Functional Predictor")+
  geom_line()+
  labs(y = TeX("$x_{30}(t)$"))+
  lims(x = c(0,1), y = c(-1.75,1.75))+
  theme_bw()+
  theme(text = element_text(size = 14))
```


## Example I: Misaligned Grids

```{r}
tibble(x = X_A_train[30,], t = grd_A) %>%
  ggplot(aes(t,x))+
  ggtitle("Observed Functional Predictor: Observation Grid A")+
  geom_line(alpha = .5)+
  geom_point()+
  labs(y = TeX("$x_{30}(t)$"))+
  lims(x = c(0,1), y = c(-1.75,1.75))+
  theme_bw()+
  theme(text = element_text(size = 14))
```

## Example I: Misaligned Grids

```{r}
tibble(x = X_B_train[30,], t = grd_B) %>%
  ggplot(aes(t,x))+
  ggtitle("Observed Functional Predictor: Observation Grid B")+
  geom_line(alpha = .5)+
  geom_point()+
  labs(y = TeX("$x_{30}(t)$"))+
  lims(x = c(0,1), y = c(-1.75,1.75))+
  theme_bw()+
  theme(text = element_text(size = 14))
```

## Example I: Misaligned Grids

```{r}

tibble(x = c(X_A_train[30,],X_B_train[30,]) , t = c(grd_A,grd_B), grid = rep(c("A","B"), c(length(grd_A), length(grd_B)))) %>%
  ggplot(aes(t,x, color = grid))+
  ggtitle("Functional Predictor on Two Observation Grids")+
  geom_line(color = 'black', alpha = .5)+
  geom_point(size = 2)+
  labs(y = TeX("$x_{30}(t)$"))+
  lims(x = c(.80,.90), y = c(0,1))+
  theme_bw()+
  theme(text = element_text(size = 14))

```


## Example I: Two Approaches

- Goal: predict $y$ from $x(t)$ observed on $G_B$, using a model trained with $x(t)$ observed on $G_A$.
  + $80/20$ train/test split.
  
Classical PLS Approach:

- Obtain PLS coefficients $\boldsymbol{\hat{\beta}}_A$ using $y^{train}$ and $x^{train}(t)$ on $G_A$
- Select PLS coefficients closest to points on $G_B$, $\boldsymbol{\hat{\beta}}_B$
- Predict $y^{test}$ using observations of $x^{test}(t)$ on $G_B$ and $\boldsymbol{\hat{\beta}}_B$


\vspace{59pt}


## Example I: Two Approaches

- Goal: predict $y$ from $x(t)$ observed on $G_B$, using a model trained with $x(t)$ observed on $G_A$.
  + $80/20$ train/test split.
  
Classical PLS Approach:

- Obtain PLS coefficients $\boldsymbol{\hat{\beta}}_A$ using $y^{train}$ and $x^{train}(t)$ on $G_A$
- Select PLS coefficients closest to points on $G_B$, $\boldsymbol{\hat{\beta}}_B$
- Predict $y^{test}$ using observations of $x^{test}(t)$ on $G_B$ and $\boldsymbol{\hat{\beta}}_B$


Functional PLS approach:

- Obtain $\hat{\beta}_{FPLS}(t)$ using observations of $x^{train}(t)$ on $G_A$
- Predict $y^{test}$ using observations of $x^{test}(t)$ on $G_B$ and $\hat{\beta}(t)$.


## Example I: Classical PLS

```{r}
tb_true <- tibble(beta = beta(grd_A), t = grd_A)
tb_pls <- tibble(beta = alpha*p1, t = grd_A)
tb_pls_adj <- tibble(beta = alpha_adj*p1, t = grd_B)

ggplot()+
  ggtitle(TeX("PLS Coefficeints ($G_A$)"))+
  geom_line(data = tb_true, aes(t,beta), alpha = .5)+
  geom_point(data = tb_pls, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))

```

## Example I: Classical PLS

```{r}
ggplot()+
  ggtitle(TeX("PLS Coefficeints (closest to $G_B$)"))+
  geom_line(data = tb_true, aes(t,beta), alpha = .5)+
  geom_point(data = tb_pls_adj, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))
```



## Example I: Classical PLS

```{r}
pmse_label_A <- glue("PMSE = {round(pmse(Y_test,pls_preds_A),4)}")
pmse_label_B <- glue("PMSE = {round(pmse(Y_test,pls_preds_B),4)}")
tb_ann <- tibble(x = c(-5,-5), y = c(15,15), label = c(pmse_label_A, pmse_label_B), grid = c("A","B"))


tibble(preds = c(pls_preds_A,pls_preds_B), y = rep(Y_test,2), grid = rep(c("A","B"), each = length(Y_test))) %>%
  ggplot(aes(y,preds))+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))
```


## Example I: Functional PLS


```{r}
tb_fpls <- tibble(beta = fplsr_fit_basis$beta_fun(grd_A), t = grd_A) 

ggplot()+
  ggtitle("FPLS Coefficient")+
  geom_line(data = tb_true, aes(t,beta), alpha = .4)+
  geom_line(data = tb_fpls, aes(t,beta))+
  labs(y = TeX("$\\hat{\\beta}(t)$"))+
  theme_bw()+
  theme(text = element_text(size = 14))


```



## Example I: Functional PLS 

```{r}
pmse_label_A_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_A_basis),4)}")
pmse_label_B_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_B_basis),4)}")
tb_ann_f <- tibble(x = c(-5,-5), y = c(15,15), label = c(pmse_label_A_f, pmse_label_B_f), grid = c("A","B"))



tibble(preds = c(fpls_preds_A_basis,fpls_preds_B_basis), y = rep(Y_test,2), grid = rep(c("A","B"), each = length(Y_test))) %>%
  ggplot(aes(y,preds))+
  ggtitle("FPLS Predictions (with basis expansion)")+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann_f, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))

```



## Example I: Functional PLS 

```{r}
pmse_label_A_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_A),4)}")
pmse_label_B_f <- glue("PMSE = {round(pmse(Y_test,fpls_preds_B),4)}")
tb_ann_f <- tibble(x = c(-5,-5), y = c(15,15), label = c(pmse_label_A_f, pmse_label_B_f), grid = c("A","B"))



tibble(preds = c(fpls_preds_A,fpls_preds_B), y = rep(Y_test,2), grid = rep(c("A","B"), each = length(Y_test))) %>%
  ggplot(aes(y,preds))+
  ggtitle("FPLS Predictions (w/out basis expansion)")+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann_f, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  theme_bw()+
  theme(text = element_text(size = 14))

```


## Example II: AOP Crown Data

```{r}
source('../example_AOP_crown_data.R')
```


We applied the same method to the AOP Crown data to predict \texttt{d15N} from spectra. After joining the site trait data and spectra by \texttt{SampleSiteID}, and removing both "bad bands" and NA observations we had:

- $n = 2515$ observations
- $p_A = 350$ spectral points per spectra.

To simulate spectral misalignment, we expanded the spectra onto a set of $52$ cubic B-splines and sampled along an observation grid of $p_B = 200$ points.

## Example II: Spectra

```{r}
tibble(x = X[1,], wavelength = good_grd) %>%
  ggplot(aes(wavelength,x))+
  ggtitle("Example Spectra")+
  labs(y = "Reflectance")+
  geom_point()+
  theme_bw()+
  theme(text = element_text(size = 14))
```

## Example II: Spectra

```{r}
tibble(x = X[1,], wavelength = good_grd) %>%
  ggplot(aes(wavelength,x))+
  ggtitle("Example Spectra: Smoothed")+
  labs(y = "Reflectance")+
  geom_line()+
  theme_bw()+
  theme(text = element_text(size = 14))

```

## Example II: Spectra

```{r}

tibble(x = X_A[1,], wavelength = grdA) %>%
  ggplot(aes(wavelength,x))+
  ggtitle("Example Spectra: Smoothed + Scaled Grid")+
  labs(y = "Reflectance")+
  geom_line()+
  theme_bw()+
  theme(text = element_text(size = 14))

```

## Example II: Spectra

```{r}

tibble(x = X_A[1,], wavelength = grdA) %>%
  ggplot(aes(wavelength,x))+
  ggtitle("Example Spectra: Instrument A")+
  labs(y = "Reflectance")+
  geom_point()+
  theme_bw()+
  theme(text = element_text(size = 14))


```


## Example II: Spectra

```{r}
tibble(x = X_B[1,], wavelength = grdB) %>%
  ggplot(aes(wavelength,x))+
  ggtitle("Example Spectra: Instrument B")+
  labs(y = "Reflectance")+
  geom_point()+
  theme_bw()+
  theme(text = element_text(size = 14))
```


## Classical PLS Approach

```{r}
pmse_label_A <- glue("PMSE = {round(pmse(y_test,pls_preds_A),4)}")
pmse_label_B <- glue("PMSE = {round(pmse(y_test,pls_preds_B),4)}")
tb_ann <- tibble(x = c(-2.5,-2.5), y = c(4,4), label = c(pmse_label_A, pmse_label_B), grid = c("A","B"))


tibble(preds = c(pls_preds_A,pls_preds_B), y = rep(y_test,2), grid = rep(c("A","B"), each = length(y_test))) %>%
  ggplot(aes(y,preds))+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  lims(y = c(-4,4))+
  theme_bw()+
  theme(text = element_text(size = 14))

```

## Functional PLS Approach

```{r}
pmse_label_A <- glue("PMSE = {round(pmse(y_test,fpls_preds_A_basis),4)}")
pmse_label_B <- glue("PMSE = {round(pmse(y_test,fpls_preds_B_basis),4)}")
tb_ann <- tibble(x = c(-2.5,-2.5), y = c(4,4), label = c(pmse_label_A, pmse_label_B), grid = c("A","B"))


tibble(preds = c(fpls_preds_A,fpls_preds_B), y = rep(y_test,2), grid = rep(c("A","B"), each = length(y_test))) %>%
  ggplot(aes(y,preds))+
  geom_point()+
  facet_wrap(~grid)+
  geom_abline(slope = 1, intercept = 0)+
  geom_text(data = tb_ann, aes(x=x,y=y, label = label))+
  labs(x = TeX("$y_{test}$"), y = TeX("$\\hat{y}_{test}$"))+
  lims(y = c(-4,4))+
  theme_bw()+
  theme(text = element_text(size = 14))
```






